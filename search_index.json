[
["index.html", "slapnap: Super LeArner Prediction of NAb Panels Welcome", " slapnap: Super LeArner Prediction of NAb Panels David Benkeser, Brian D. Williamson, Craig A. Magaret, Sohail Nizam, Courtney Simmons, Peter B. Gilbert July 23, 2021 Welcome The slapnap container is a tool for using the Compile, Analyze and Tally NAb Panels (CATNAP; Yoon et al. 2015) database to develop predictive models of HIV-1 neutralization sensitivity to one or several broadly neutralizing antibodies (bnAbs). Crystal structure of HIV-1 gp120 glycoprotein. Highlighted residues indicating sites most-predictive of VRC01 neutralization resistance. (Magaret et al. 2019) The tool can be used for a wide variety of tasks. In its simplest form, slapnap can be used simply to access and format data from CATNAP in a way that is usable for machine learning analysis. The tool also offers fully automated and customizable machine learning analyses based on up to five different neutralization endpoints, complete with automated report generation to summarize results and identify the most predictive features. This document serves as the user manual for the slapnap container. Here, we describe everything needed to utilize slapnap and understand its output. The documentation is organized into the following sections: Section 1 provides a brief overview of Docker, including information on installing Docker and downloading the slapnap container. Section 2 provides a brief overview of the CATNAP database and the details of how and when these data were accessed to build the slapnap container. Section 3 provides a detailed description of how to make calls to slapnap and all options that are available at run time to customize its behavior. Section 4 includes example calls to slapnap for accomplishing different tasks. Section 5 describes the methodology used by slapnap to generate and analyze data. Section 6 describes the contents of the automated report generated by slapnap. Section 7 provides a description of the analysis data set created by slapnap. If you have any issues or questions about using slapnap, please file an issue on GitHub. References "],
["1-sec-docker.html", "1 Docker", " 1 Docker Docker is a free platform for building containers. Containers are standard units of software that package together code and dependencies, so that the code can be executed reliably irrespective of computing environment. slapnap relies on machine learning implemented in the R language and relies on several packages. Achieving full reproducibility for such analyses is challenging in that it requires synchronization across the specific version of R and dependent packages. In other words, two users running two versions of R or two versions of the same R package may arrive at different output when running the same code. Containerization ensures that this does not happen. Any two runs of the same built slapnap container with the same input options will yield the same output every time. Installing Docker is necessary for running slapnap. While it is not necessary for execution of the slapnap container, readers interested in learning more about Docker should consult the Docker documentation for information about getting started using Docker. Once Docker has been installed on your local computer, you can download slapnap using the following command. docker pull slapnap/slapnap This command pulls an image (a built slapnap container) from DockerHub. Once the image has been downloaded, we are ready to learn about how to execute slapnap jobs. The next section contains information on the source data used by slapnap. Users familiar with the CATNAP data may wish to skip directly to Section 3. "],
["2-sec-catnap.html", "2 CATNAP", " 2 CATNAP The CATNAP database is a web server hosted by Los Alamos National Laboratory (Yoon et al. 2015). The database integrates antibody neutralization and HIV-1 sequence data from published studies. Neutralization is measured in terms of half-maximal inhibitory concentration (IC\\(_{50}\\)) and 80% inhibitory concentration (IC\\(_{80}\\)). These measures of neutralization against HIV envelope pseudoviruses are available for many broadly neutralizing antibodies (bnAbs) and for some combination bnAbs. Also available on each pseudovirus are amino acid (AA) sequence features for the gp160 protein. These are detailed in Section 7. During each build of the slapnap container, all raw data are downloaded from CATNAP. At run time, pseudovirus features are derived and measured sensitivity outcomes are derived from the raw CATNAP database files and merged into a .csv file that is used in subsequent predictive analyses. The CATNAP data are updated periodically. The data are downloaded into the slapnap container at every build. The most recent build occurred on July 23, 2021. References "],
["3-sec-runningcontainer.html", "3 Running slapnap 3.1 slapnap run options 3.2 Returning output", " 3 Running slapnap To run the slapnap container, we make use of the docker run command. Note that administrator (sudo) privileges are needed to execute this command. Additionally, note that slapnap operates in UTC+0 time – this will be important when inspecting the files generated by slapnap. There are several options that are necessary to include in this command to control the behavior of slapnap. These are discussed in separate subsections below. 3.1 slapnap run options The user has control over many aspects of slapnap’s behavior. These options are passed in using the -e option1. Semicolon-separated strings are used to set options. For example, to provide input for the option option_name, we would used -e option_name=\"a;semicolon;separated;string\". Note that there are no spaces between the option name and its value and no spaces after semicolons in the separated list. See Section 4 for full syntax. Each description below lists the default value that is assumed if the option is not specified. Note that many of the default values are chosen simply so that naive calls to slapnap compile quickly. Proper values should be determined based on scientific context. -e options for slapnap nab: A semicolon-separated list of bnAbs (default = \"VRC01\"). A list of possible bnAbs can be found here. If multiple bnAbs are listed, it is assumed that the analysis should be of estimated outcomes for a combination of bnAbs (see Section 5.1 for details on how estimated outcomes for multiple bnAbs are computed). outcomes: A semicolon-separated string of outcomes to include in the analysis. Possible values are \"ic50\" (included in default), \"ic80\", \"iip\", \"sens\" (included in default), \"estsens\", and \"multsens\". If only a single nab is specified, use sens to include a dichotomous endpoint. If multiple nabs are specified, use estsens and/or multsens. For detailed definitions of outcomes see Section 5.1. combination_method A string defining the method to use for predicting combination IC\\(_{50}\\) and/or IC\\(_{80}\\). Possible values are \"additive\" (the default, for the additive model defined in Wagh et al. 2016) or \"Bliss-Hill\" (for the Bliss-Hill model defined in Wagh et al. 2016). binary_outcomes A string defining the measure of neutralization to use for defining binary outcomes. Possible values are \"ic50\" (the default, for using IC\\(_{50}\\) to define sensitivity) or \"ic80\" (for using IC\\(_{80}\\) to define sensitivity). sens_thresh A numeric value defining the neutralization threshold for defining a sensitive versus resistant pseudovirus (default = 1). The dichotomous sensitivity/resistant outcome is defined as the indicator that (estimated) IC\\(_{50}\\) (or IC\\(_{80}\\), if binary_outcomes=\"ic80\") is greater than or equal to sens_thresh. multsens_nab A numeric value used for defining whether a pseudovirus is resistant to a multi-nAb cocktail. Only used if multsens is included in outcomes and more than one nab is requested. The dichotomous outcome multsens is defined as the indicator that a virus has IC\\(_{50}\\) (or IC\\(_{80}\\), if binary_outcomes=\"ic80\") greater than sens_thresh for at least multsens_nab nAbs. learners: A semicolon-separated string of machine learning algorithms to include in the analysis. Possible values include \"rf\" (random forest, default), \"xgboost\" (eXtreme gradient boosting), \"h2oboost\" (gradient boosting using H2O.ai) and \"lasso\" (elastic net). See Section 5.2 for details on how tuning parameters are chosen. If more than one algorithm is included, then it is assumed that a cross-validated-based ensemble (i.e., a super learner) is desired (see Section 5.3). cvtune: A boolean string (i.e., either \"TRUE\" or \"FALSE\" [default]) indicating whether the learners should be tuned using cross validation and a small grid search. Defaults to \"FALSE\". If multiple learners are specified, then the super learner ensemble includes up to three versions of each of the requested learners with different tuning parameters. cvperf: A boolean string (i.e., either \"TRUE\" or \"FALSE\" [default]) indicating whether the learners performance should be evaluated using cross validation. If cvtune=\"TRUE\" or learners includes multiple algorithms, then nested cross validation is used to evaluate the performance of the cross validation-selected best value of tuning parameters for the specified algorithm or the super learner, respectively. var_thresh: A numeric string that defines a threshold for pre-screening features. If a single positive number, all binary features with fewer than var_thresh 0’s or 1’s are removed prior to the specified learner training. If several values are included in var_thresh and a single learner is specified, then cross-validation is used to select the optimal threshold. If multiple learners are specified, then each learner is included in the super learner with pre-screening based on each value of var_thresh. nfolds: A numeric string indicating the number of folds to use in cross validation procedures (default = \"2\"). importance_grp: A semicolon-separated string indicating which group-level variable importance measures should be computed. Possible values are none \"\" (default), marginal \"marg\", conditional \"cond\". See Section 5.4.1 for details on these measures. importance_ind: A semicolon-separated string indicating which individual-level variable importance measures should be computed. Possible values are none \"\" (default), learner-level \"pred\", marginal \"marg\" and conditional \"cond\". The latter two take significant computation time to compute. See Sections 5.4.1 and 5.4.2 for details on these measures. same_subset If \"FALSE\" (default) all data available for each outcome will be used in the analysis. If \"TRUE\", when multiple outcomes are requested, the data will be subset to just those sequences that have all measured outcomes, and, if iip is requested, for which iip can be computed (i.e., measured IC\\(_{50}\\) and IC\\(_{80}\\) values are different). Thus, if \"TRUE\" all requested outcomes will be evaluated using the same_subset of the CATNAP data. report_name: A string indicating the desired name of the output report (default = report_[_-separated list of nabs]_[date].html). return: A semicolon-separated string of the desired output. Possible values are \"report\" (default), \"learner\" for a .rds object that contains the algorithm for each endpoint trained using the full analysis data, \"data\" for the analysis dataset, \"figures\" for all figures from the report, and \"vimp\" for variable importance objects. view_port: A boolean string indicating whether the compiled report should be made viewable on localhost (default \"FALSE\"). If \"TRUE\" then -p option should be used in the docker run command to identify the port. See example in Section 4.2 for details. 3.2 Returning output At the end of a slapnap run, user-specified output will be saved (see option return in Section 3.1). To retrieve these files from the container, there are two options: mounting a local directory (Section 3.2.1) or, if the report is the only desired output, viewing and saving the report in a web browser (Section 3.2.2). 3.2.1 Mounting a local directory To mount a local directory to the output directory in the container (/home/output/), use the -v option. Any items saved to the output directory in the container (file path in the container /home/output/) will be available in the mounted directory. Conversely, all files in the mounted local directory will be visible to programs running inside the container. Suppose /path/to/local/dir is the file path on a local computer in which we wish to save the output files from a slapnap run. A docker run of slapnap would include the option -v /path/to/local/dir:/home/output. After a run completes, the requested output should be viewable in /path/to/local/dir. See Section 4 for full syntax. To avoid possible naming conflicts and file overwrites in the mounted directory, we recommend mounting an empty directory to store the output. Widows users need to enable shared drives by clicking Settings &gt; Shared Drives in the Docker Desktop Daemon and sharing the drive that contains path/to/local/dir. 3.2.2 Viewing report in browser An alternative option to mounting local directories for viewing and downloading the report is to set the view_port option to \"TRUE\" and open a port to the container via the -p option in the docker run statement. In this case, rather than exiting upon completion of the analysis, the container will continuing to run and broadcast the compiled report to localhost at the specified port (see examples below). The report can be downloaded from the web browser directly in this way. References "],
["4-sec-examples.html", "4 Examples 4.1 Basic call to slapnap 4.2 Viewing report in browser 4.3 Super learning 4.4 Train an algorithm 4.5 Pull and clean data 4.6 Interactive sessions", " 4 Examples 4.1 Basic call to slapnap A call to slapnap with all default options can be run using the following command. docker run -v /path/to/local/dir:/home/output slapnap/slapnap Note that this call mounts the local directory path/to/local/dir to receive output from the container (see Section 3.2.1). When this command is executed, messages will print to indicate the progress of the container. The first message will report the name of the log file, which will appear in /path/to/local/dir (note that the name of the log file is based on the current time, which is in UTC+0). The container will then compile an analytic data set from the CATNAP database for the default bnAb (VRC01), train the default learner (random forest) for the default outcomes (ic50 and sens), evaluate its performance using two-fold (default for nfolds) cross validation, and compile a report detailing the results, and place the compiled report in path/to/local/dir (note that the default name of the report is based on the current time, which is in UTC+0). 4.2 Viewing report in browser To have the results viewable in a web browser execute the following command2. docker run -v /path/to/local/dir:/home/output \\ -e view_port=&quot;TRUE&quot; -p 80:80 \\ slapnap/slapnap This command opens port 80 on the container. Once the report has been compiled, the container will not close down automatically. Instead it will continue to run, broadcasting the report to port 80. Open a web browser on your computer and navigate to localhost:80 and you should see the compiled report. Many web browsers should allow downloading of the report (e.g., by right-clicking and selecting save Save As...). The container will continue to run until you tell it to stop. To do that, retrieve the container ID by executing docker container ps3. Copy the ID of the running container, which will be a string of numbers and letters (say a1b2c3d4) and then execute docker stop a1b2c3d4 to shut down the container. Note that in the above command, we have still mounted a local directory, which may be considered best practice in case other output besides the report is desired to be returned. 4.3 Super learning If multiple learners are specified, then a super learner ensemble (van der Laan, Polley, and Hubbard 2007) is constructed based on the requested learners and a predictor that simply returns the empirical average of the outcome (i.e., ignores all features entirely). In the following command, we construct an ensemble based on a random forest (Breiman 2001) and elastic net (Zou and Hastie 2005). Note that the execution time for super learners can be considerably greater than for single learners because of the extra layer of cross validation needed to construct the ensemble. docker run -v /path/to/local/dir:/home/output \\ -e learners=&quot;rf;lasso&quot; \\ slapnap/slapnap For specific details on the super learner algorithms implemented in slapnap, see Section 5.3. 4.4 Train an algorithm The previous commands train learners and evaluate their performance using cross validation. However, at times we may wish only to use slapnap to train a particular algorithm, while avoiding the additional computational time associated with evaluating its cross-validated performance and compiling a report. We show an example of this below using sensitivity as the outcome. docker run -v /path/to/local/dir:/home/output \\ -e learners=&quot;rf&quot; \\ -e return=&quot;learner&quot; \\ -e cvperf=&quot;FALSE&quot; \\ -e outcomes=&quot;sens&quot; \\ slapnap/slapnap After completion of this run, learner_sens.rds will appear in /path/to/local/dir that contains an R object of class ranger (the R package used by slapnap to fit random forests). You can confirm this from the command line by executing Rscript -e &quot;learner &lt;- readRDS(&#39;/path/to/local/dir/learner_sens.rds&#39;); class(learner)&quot; 4.5 Pull and clean data The slapnap container can also be used to return cleaned CATNAP data suitable for analyses not supported by the slapnap pipeline. In this case, the container avoids training machine learning algorithms and report generation, returning a data set and associated documentation. In the following call, return only includes \"data\"; thus, options pertaining to the machine learning portions of slapnap are essentially ignored by slapnap. The inputted outcomes are also irrelevant, as all outcomes are included in the resultant data set. docker run -v /path/to/local/dir:/home/output \\ -e return=&quot;data&quot; \\ slapnap/slapnap Note that the data set returned by slapnap contains the outcomes used by slapnap; in other words, (estimated) IC\\(_{50},\\) (estimated) IC\\(_{80}\\), and IIP are all log-transformed (see Section 5.1 for more details). 4.6 Interactive sessions To simply enter and explore the container, use an interactive session by including -it and overriding the container’s entry point. docker run -it slapnap/slapnap /bin/bash This will enter you into the container in a bash terminal prior to any portions of the analysis being run. This may be useful for exploring the file structure, examining versions of R packages that are included in the container, etc. To enter the container interactively after the analysis has run, you can execute the following commands. Here we add the -d option to start the container in detached mode. docker run -d -p 80:80 -e view_port=&quot;TRUE&quot; slapnap/slapnap # ...wait for analysis to finish... # use this command to enter the container docker exec -it /bin/bash To close the interactive session type exit at the container command prompt and hit Return. This will close the container and stop its running. References "],
["5-sec-methods.html", "5 Methods 5.1 Outcomes 5.2 Learners 5.3 Super learner 5.4 Variable importance", " 5 Methods 5.1 Outcomes 5.1.1 Single bnAb If a single bnAb or combination of bnAbs that are measured directly in the CATNAP data is requested (i.e., the nab option is a single string of a single bnAb or measured combination of bnAbs from the CATNAP database), then the possible outcomes are: ic50 = \\(\\mbox{log}_{10}(\\mbox{IC}_{50})\\), where IC\\(_{50}\\) is the half-maximal inhibitory concentration; ic80 = \\(\\mbox{log}_{10}(\\mbox{IC}_{80})\\), where IC\\(_{80}\\) is the 80% maximal inhibitory concentration; iip = \\((-1)\\mbox{log}_{10}(1 - \\mbox{IIP})\\), where IIP (Shen et al. 2008, @wagh2016optimal) is the instantaneous inhibitory potential, computed as \\[ \\frac{10^m}{\\mbox{IC$_{50}$}^m + 10^m} \\ , \\] where \\(m = \\mbox{log}_{10}(4) / (\\mbox{log}_{10}(\\mbox{IC}_{80}) - \\mbox{log}_{10}(\\mbox{IC}_{50}))\\); and sens = sensitivity: the binary indicator that IC\\(_{x}\\) \\(&lt;\\) sens_thresh, the user-specified sensitivity threshold. The value of \\(x\\) is determined by binary_outcomes, which defaults to \"ic50\" (i.e., \\(x = 50\\)) but may be set to \"ic80\" (i.e., \\(x = 80\\)). 5.1.2 Multiple bnAbs If multiple bnAbs are requested (i.e., the nab option is a semicolon separated string of more than one bnAb from CATNAP), then the possible outcomes that can be requested are: ic50 = \\(\\mbox{log}_{10}(\\mbox{estimated IC}_{50})\\), where estimated IC\\(_{50}\\) is computed using the requested combination_method (see below); ic80 = \\(\\mbox{log}_{10}(\\mbox{estimated IC}_{80})\\), where estimated IC\\(_{80}\\) is computed using the requested combination_method (see below); iip = \\((-1)\\mbox{log}_{10}(1 - \\mbox{IIP})\\), where IIP is computed as \\[ \\frac{10^m}{\\mbox{estimated IC$_{50}$}^m + 10^m} \\ , \\] where \\(m = \\mbox{log}_{10}(4) / (\\mbox{log}_{10}(\\mbox{estimated IC}_{80}) - \\mbox{log}_{10}(\\mbox{estimated IC}_{50}))\\); and estsens = estimated sensitivity: the binary indicator that estimated IC\\(_{x}\\) (defined above) is less than sens_thresh (where \\(x\\) is determined by the value of binary_outcomes); and multsens = multiple sensitivity: the binary indicator that measured IC\\(_{x}\\) is less than the sensitivity threshold (sens_thresh) for a number of bnAbs defined by multsens_nab (where \\(x\\) is determined by the value of binary_outcomes). Possible combination_methods used for computing estimated IC\\(_{50}\\) and IC\\(_{80}\\) are: additive, the additive model of Wagh et al. (2016). For \\(J\\) bnAbs, \\[ \\mbox{estimated IC}_{50} = \\left( \\sum_{j=1}^J \\mbox{IC}_{50,j}^{-1} \\right)^{-1} \\ , \\] where IC\\(_{50,j}\\) denotes the measured IC\\(_{50}\\) for antibody \\(j\\); Bliss-Hill, the Bliss-Hill model of Wagh et al. (2016). For \\(J\\) bnAbs, computed using Brent’s algorithm (Brent 1971) as the concentration value \\(c\\) that minimizes \\(\\lvert f_J(c) - k \\rvert\\), where \\(k\\) denotes the desired neutralization fraction (50% for IC\\(_{50}\\) or 80% for IC\\(_{80}\\)), \\[ f_J(c) = 1 - \\prod_{j = 1}^J \\{1 - f_j(c, c / J)\\}; \\] \\[ f_j(c, c_j) = (c^m) / (\\mbox{IC}_{50,j}^m + c_j^m), \\] \\(m = \\mbox{log}_{10}(4) / (\\mbox{log}_{10}(\\mbox{IC}_{80,j}) - \\mbox{log}_{10}(\\mbox{IC}_{50,j}))\\), and IC\\(_{50,j}\\) and IC\\(_{80,j}\\) denote the measured IC\\(_{50}\\) and IC\\(_{80}\\) for bnAb \\(j\\), respectively. 5.2 Learners There are three possible learners available in slapnap: random forests (Breiman 2001), as implemented in the R package ranger (Wright and Ziegler 2017) and accessed in slapnap by including 'rf' in learners; elastic net (Zou and Hastie 2005) as implemented in glmnet (Friedman, Hastie, and Tibshirani 2010) and accessed in slapnap by including 'lasso' in learners; and boosted trees (Friedman 2001; Chen and Guestrin 2016) as implemented in either xgboost (Chen et al. 2019) (accessed in slapnap by including 'xgboost' in learners) or H2O.ai (H2O.ai 2016) (accessed in slapnap by including 'h2oboost' in learners). For each learner, there is a default choice of tuning parameters that is implemented if cvtune=\"FALSE\". If instead cvtune=\"TRUE\", then there are several choices of tuning parameters that are evaluated using nfold cross validation; see Table 5.1 for a full list of these learners. Table 5.1: Labels for learners in report and description of their respective tuning parameters learner Tuning parameters rf_default mtry equal to square root of number of predictors rf_1 mtry equal to one-half times square root of number of predictors rf_2 mtry equal to two times square root of number of predictors xgboost_default maximum tree depth equal to 4 xgboost_1 maximum tree depth equal to 2 xgboost_2 maximum tree depth equal to 6 xgboost_3 maximum tree depth equal to 8 h2oboost_default max_depth in (2, 4, 5, 6), learn_rate in (.05, .1, .2), and col_sample_rate in (.1, .2, .3); optimal combination chosen via 5-fold CV lasso_default \\(\\lambda\\) selected by 5-fold CV and \\(\\alpha\\) equal to 0 lasso_1 \\(\\lambda\\) selected by 5-fold CV and \\(\\alpha\\) equal to 0.25 lasso_2 \\(\\lambda\\) selected by 5-fold CV and \\(\\alpha\\) equal to 0.5 lasso_3 \\(\\lambda\\) selected by 5-fold CV and \\(\\alpha\\) equal to 0.75 Tuning parameters not mentioned in the table are set as follows: rf: num.trees = 500, min.node.size = 5 for continuous outcomes and = 1 for binary outcomes; xgboost: nrounds = 1000, eta = 0.1, min_child_weight = 10, objective = binary:logistic for binary outcomes and objective = reg:squarederror for continuous outcomes. h2oboost: ntrees = 1000; for binary outcomes, distribution = \"bernoulli\", balance_classes = TRUE, fold_assignment = \"Stratified\", stopping_metric = \"AUC\", while for continuous outcomes, distribution = \"gaussian\", balance_classes = FALSE, fold_assignment = \"AUTO\", stopping_metric = \"MSE\"; and max_after_balance_class_size = 5, stopping_rounds = 3, stopping_tolerance = 0.001, max_runtime_secs = 60. 5.3 Super learner If multiple learners are specified, then a super learner ensemble (van der Laan, Polley, and Hubbard 2007) is constructed using nfold cross validation, as implemented in the R package SuperLearner (Polley et al. 2019). Specifically, the data are randomly partitioned into nfold chunks of approximately equal size. For binary outcomes, this partitioning is done in such a way as to ensure an approximately even number of sensitive/resistant pseudoviruses in each chunk. A so-called super learner library of candidate algorithms is constructed by including different learners: the algorithm mean, which reports back the sample mean as prediction for all observations is always included; if cvtune=\"FALSE\" then the default version of each learner (Section 5.2) is included; if cvtune=\"TRUE\" then each choice of tuning parameters for the selected learners in Table 5.1 is included. The cross-validated risk of each algorithm in the library is computed. For binary outcomes, mean negative log-likelihood loss is used; for continuous outcomes, mean squared-error loss is used. The single algorithm with the smallest cross-validated risk is reported as the cv selector (also known as the discrete super learner). The super learner ensemble is constructed by selecting convex weights (i.e., each algorithm is assigned a non-negative weight and the weights sum to one) that minimize cross-validated risk. When cvperf=\"TRUE\" and a super learner is constructed, an additional layer of cross validation is used to evaluate the predictive performance of the super learner and of the cv selector. 5.4 Variable importance If importance_grp or importance_ind is specified, variable importance estimates are computed based on the learners. Both intrinsic and prediction importance can be obtained; we discuss each in the following two sections. 5.4.1 Intrinsic importance Intrinsic importance may be obtained by specifying importance_grp, importance_ind, or both. We provide two types of intrinsic importance: marginal and conditional, accessed by passing \"marg\" and \"cond\", respectively, to one of the importance options. Both types of intrinsic importance are based on the population prediction potential of features (Williamson et al. 2020), as implemented in the R package vimp (B. D. Williamson, Simon, and Carone 2020). We measure prediction potential using nonparametric \\(R^2\\) for continuous outcomes [i.e., (estimated) IC\\(_{50}\\), (estimated) IC\\(_{80}\\), or IIP] and using the nonparametric area under the receiver operating characteristic curve (AUC) for binary outcomes [i.e., (estimated) sensitivity or multiple sensitivity]. Both marginal and conditional importance compare the population prediction potential including the feature(s) of interest to the population prediction potential excluding the feature(s) of interest; this provides a measure of the intrinsic importance of the feature(s). The two types of intrinsic importance differ only in the other adjustment variables that we consider: conditional importance compares the prediction potential of all features to the prediction potential of all features excluding the feature(s) of interest, and thus importance must be interpreted conditionally; whereas marginal importance compares the prediction potential of the feature(s) of interest plus geographic confounders to the prediction potential of the geographic confounders alone. Both marginal and conditional intrinsic importance can be computed for groups of features or individual features. The available feature groups are detailed in Section 7. Execution time may increase when intrinsic importance is requested, depending upon the other options passed to slapnap: a separate learner (or super learner ensemble) must be trained for each feature group (or individual feature) of interest. Marginal importance tends to be computed more quickly than conditional importance, but both types of importance provide useful information about the population of interest and the underlying biology. If intrinsic importance is requested, then point estimates, confidence intervals, and p-values (for a test of the null hypothesis that the intrinsic importance is equal to zero) will be computed and displayed for each feature or group of features of interest. All results are based on an internal sample-splitting procedure, whereby the algorithm including the feature(s) of interest is evaluated on independent data from the algorithm excluding the feature(s) of interest. This ensures that the procedure has a type I error rate of 0.05 (Williamson et al. 2020). In the following command, we request marginal intrinsic importance for the feature groups defined in Section 7. We do not specify a super learner ensemble to reduce computation time; however, in most problems we recommend an ensemble to protect against model misspecification. docker run -v /path/to/local/dir:/home/output \\ -e importance_grp=&quot;marg&quot; \\ slapnap/slapnap The raw R objects (saved as .rds files) containing the point estimates, confidence intervals, and p-values for intrinsic importance can be saved by passing \"vimp\" to return. 5.4.2 Predictive importance learner-level predictive importance may be obtained by including \"pred\" in the importance_ind option. If a single learner is fit, then the predictive importance is the R default for that type of learner: rf: the impurity importance from ranger (Wright and Ziegler 2017) is returned. The impurity importance for a given feature is computed by taking a normalized sum of the decrease in impurity (i.e., Gini index for binary outcomes; mean squared-error for continuous outcomes) over all nodes in the forest at which a split on that feature has been conducted. xgboost: the gain importance from xgboost (Chen et al. 2019) is returned. Interpretation is essentially the same as for rf’s impurity importance. h2oboost: the gain importance (H2O.ai 2016) is returned. Interpretation is the same as for xgboost’s gain importance. lasso: the absolute value of the estimated regression coefficient at the cross-validation-selected \\(\\lambda\\) is returned. Note that these importance measures each have important limitations: the rf , h2oboost, and xgboost measures will tend to favor features with many levels, while the lasso variable importance will tend to favor features with few levels. Nevertheless, these commonly reported measures can provide some insight into how a given learner is making predictions. If multiple learners are used, and thus a super learner is constructed, then the importance measures for the learner with the highest weight in the super learner are reported. If a single learner is used, but cvtune=\"TRUE\" then importance measures for the cv selector are reported. In the following command, we request predictive importance for a simple scenario. Predictive importance is displayed for the top 15 features. docker run -v /path/to/local/dir:/home/output \\ -e importance_ind=&quot;pred&quot; \\ slapnap/slapnap References "],
["6-sec-report.html", "6 Report 6.1 General structure 6.2 Example reports", " 6 Report 6.1 General structure The slapnap report consists of an executive summary followed by results for each requested outcome. The executive summary contains: descriptions of outcomes (including how any derived outcomes are generated); descriptive statistics detailing the number of sequences extracted from CATNAP, the number of sequences with complete feature and outcome information, and the number of estimated sensitive and resistant sequences (defined based on sensitivity, estimated sensitivity, and/or multiple sensitivity); a table describing the learners used to predict each outcome; a table of cross-validated prediction performance for each outcome (if cvperf = TRUE); a table of ranked marginal intrinsic prediction performance for each feature group and outcome (if \"marg\" is included in importance_grp); and a table of ranked conditional intrinsic prediction performance for each feature group and outcome (if \"cond\" is included in importance_grp). The rest of the report is organized by outcome. Each of these sections contains descriptive statistics including summaries of the distribution of the outcome (raw and log-transformed) for each bnAb for continuous outcomes and number sensitive/resistant for binary outcomes. Based on the specific options passed to slapnap, the following subsections may also be present: a table of super learner weights (Section 5.3) if an ensemble is used (i.e., multiple learners are requested); cross-validated prediction performance for the fitted learner (or super learner), if cvperf=\"TRUE\": figures showing cross-validated prediction performance (all outcomes), cross-validated receiver operating characteristic (ROC) curves (binary outcomes), and cross-validated predicted probabilities of resistance (binary outcomes); and variable importance (if importance_ind and/or importance_grp is specified): intrinsic importance (group and/or individual) and/or predictive importance. Finally, if group intrinsic importance is requested, then the variable groups are displayed in a section immediately preceding the references. 6.2 Example reports Here we include several example reports and the slapnap container run commands that generated them. 6.2.1 Single antibodies The following code evaluates binary sensitivity (defined as the indicator that IC\\(_{80} &lt; 1\\)) for VRC01 using a super learner that includes all three learner types, each with multiple tuning parameter values, and with different variable screening techniques. We also request marginal group and individual intrinsic importance and individual predictive importance. If running this command locally, change docker_output_directory to the path to the folder where the output is to be saved. See the report sudo docker run \\ -d \\ -v docker_output_directory:/home/output \\ -e nab=&quot;VRC01&quot; \\ -e outcomes=&quot;ic80;sens&quot; \\ -e binary_outcomes=&quot;ic80&quot; \\ -e learners=&quot;rf;lasso;xgboost&quot; \\ -e sens_thresh=&quot;1&quot; \\ -e var_thresh=&quot;0;4;8&quot; \\ -e nfolds=&quot;5&quot; \\ -e cvtune=&quot;TRUE&quot; \\ -e cvperf=&quot;TRUE&quot; \\ -e importance_grp=&quot;marg&quot; \\ -e importance_ind=&quot;marg;pred&quot; \\ -e return=&quot;report&quot; \\ slapnap/slapnap The next code chunk evaluates binary sensitivity (defined as the indicator that IC\\(_{50} &lt; 50\\)) for 10-1074 using a super learner that includes all three learner types, each with multiple tuning parameter values. If running this command locally, change docker_output_directory to the path to the folder where the output is to be saved. See the report sudo docker run \\ -d -v docker_output_directory:/home/output/ \\ -e nab=&quot;10-1074&quot; \\ -e outcomes=&quot;sens&quot; \\ -e learners=&quot;rf;lasso;xgboost&quot; \\ -e sens_thresh=&quot;50&quot; \\ -e nfolds=&quot;5&quot; \\ -e cvtune=&quot;TRUE&quot; \\ -e cvperf=&quot;TRUE&quot; \\ -e return=&quot;report&quot; \\ slapnap/slapnap 6.2.2 Multiple antibodies The following code evaluates binary sensitivity outcomes for a combination antibody using a super learner that includes all three learner types, each with multiple tuning parameter values, and with different variable screening techniques. If running this command locally, change docker_output_directory to the path to the folder where output is to be saved. See the report. docker run \\ -d \\ -v docker_output_directory:/home/output \\ -e learners=&quot;rf;lasso;xgboost&quot; \\ -e cvperf=&quot;TRUE&quot; \\ -e cvtune=&quot;TRUE&quot; \\ -e nab=&quot;10-1074;PG9&quot; \\ -e outcomes=&quot;estsens;multsens&quot; \\ -e sens_thresh=&quot;1&quot; \\ -e var_thresh=&quot;0;4&quot; \\ -e return=&quot;report&quot; \\ -e nfolds=&quot;5&quot; \\ slapnap/slapnap "],
["7-sec-data.html", "7 Data", " 7 Data The analysis dataset includes neutralization outcomes for the requested bnAb(s) and AA sequence features for the gp160 protein. The possible outcomes are described in Section 5.1. The additional groups of variables in the data include: geographic information: binary indicator variables describing the region of origin of each pseudovirus; subtype: binary indicator variables denoting the HIV-1 subtype for the given pseudovirus; AA sequence features: binary indicator variables denoting: presence or absence of a residue containing a specific AA at each HXB2-referenced site in gp160, the site having a leading AA for the canonical N-linked glycosylation motif (N[!P]{S/T]), an observed gap at this site after alignment to maintain site-specific relevance, or a gap at this site that resulted in a frameshift; viral geometry features: length of Env, gp120, V2, V3, V5; numbers of sequons: number of sequons in Env, gp120, V2, V3, V5; and numbers of cysteines: number of cysteines in Env, gp120, V2, V3, V5. "],
["8-sec-refs.html", "8 References", " 8 References Breiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. https://doi.org/10.1023/A:1010933404324. Brent, Richard P. 1971. “An Algorithm with Guaranteed Convergence for Finding a Zero of a Function.” The Computer Journal 14 (4): 422–25. Chen, Tianqi, and Carlos Guestrin. 2016. “Xgboost: A Scalable Tree Boosting System.” In Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining, 785–94. https://doi.org/10.1145/2939672.2939785. Chen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2019. xgboost: Extreme Gradient Boosting. https://CRAN.R-project.org/package=xgboost. Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” Annals of Statistics, 1189–1232. https://doi.org/10.1214/aos/1013203451. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software 33 (1): 1–22. https://doi.org/10.18637/jss.v033.i01. H2O.ai. 2016. R Interface for H2o. https://github.com/h2oai/h2o-3. Magaret, Craig A, David C Benkeser, Brian D Williamson, Bhavesh R Borate, Lindsay N Carpp, Ivelin S Georgiev, Ian Setliff, et al. 2019. “Prediction of VRC01 Neutralization Sensitivity by HIV-1 gp160 Sequence Features.” PLoS Computational Biology 15 (4): e1006952. https://doi.org/10.1371/journal.pcbi.1006952. Polley, Eric, Erin LeDell, Chris Kennedy, and Mark van der Laan. 2019. SuperLearner: Super Learner Prediction. https://CRAN.R-project.org/package=SuperLearner. Shen, Lin, Susan Peterson, Ahmad R Sedaghat, Moira A McMahon, Marc Callender, Haili Zhang, Yan Zhou, et al. 2008. “Dose-Response Curve Slope Sets Class-Specific Limits on Inhibitory Potential of anti-HIV Drugs.” Nature Medicine 14 (7): 762–66. https://doi.org/10.1038/nm1777. van der Laan, Mark J, Eric C Polley, and Alan E Hubbard. 2007. “Super Learner.” Statistical Applications in Genetics and Molecular Biology 6 (1). https://doi.org/10.2202/1544-6115.1309. Wagh, Kshitij, Tanmoy Bhattacharya, Carolyn Williamson, Alex Robles, Madeleine Bayne, Jetta Garrity, Michael Rist, et al. 2016. “Optimal Combinations of Broadly Neutralizing Antibodies for Prevention and Treatment of HIV-1 Clade C Infection.” PLoS Pathogens 12 (3). https://doi.org/10.1371/journal.ppat.1005520. Williamson, Brian D, Peter B Gilbert, Noah R Simon, and Marco Carone. 2020. “A Unified Approach for Inference on Algorithm-Agnostic Variable Importance.” arXiv Preprint. https://arxiv.org/abs/2004.03683. Williamson, Brian D., Noah Simon, and Marco Carone. 2020. “vimp: Perform Inference on Algorithm-Agnostic Variable Importance.” https://CRAN.R-project.org/package=vimp. Wright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01. Yoon, Hyejin, Jennifer Macke, Anthony P West Jr, Brian Foley, Pamela J Bjorkman, Bette Korber, and Karina Yusim. 2015. “CATNAP: A Tool to Compile, Analyze and Tally Neutralizing Antibody Panels.” Nucleic Acids Research 43 (W1): W213–W219. https://doi.org/10.1093/nar/gkv404. Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20. https://doi.org/10.1111/j.1467-9868.2005.00503.x. "]
]
